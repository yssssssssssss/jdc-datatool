# å¤§æ¨¡å‹äº¤äº’é€»è¾‘
from openai import OpenAI
import json
import logging
import pandas as pd
from typing import Dict, List, Any, Optional
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class LLMAnalyzer:
    def __init__(self, api_key=None, model="gpt-4o-0806", base_url=None):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model or os.getenv('OPENAI_MODEL', 'gpt-4o-0806')
        self.base_url = base_url or os.getenv('OPENAI_BASE_URL')
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if self.api_key:
            if self.base_url:
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            else:
                self.client = OpenAI(api_key=self.api_key)
        else:
            self.client = None
        
        if not self.client:
            logging.warning("OpenAI API key not found. AI features will be disabled.")
    
    def chat_with_data(self, user_question: str, data_context: Dict, chat_history: List[Dict] = None) -> Dict:
        """ä¸æ•°æ®è¿›è¡Œæ™ºèƒ½å¯¹è¯"""
        if not self.client:
            return {
                "success": False,
                "error": "OpenAI APIæœªé…ç½®",
                "response": "æŠ±æ­‰ï¼ŒAIåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·æ£€æŸ¥OpenAI APIé…ç½®ã€‚"
            }
        
        try:
            # æ„å»ºæ•°æ®ä¸Šä¸‹æ–‡
            context_info = f"""
æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š
- æ•°æ®ç»´åº¦ï¼š{data_context.get('shape', 'N/A')}
- åˆ—åï¼š{', '.join(data_context.get('columns', []))}
- æ•°å€¼å‹åˆ—ï¼š{', '.join(data_context.get('numeric_columns', []))}
- åˆ†ç±»å‹åˆ—ï¼š{', '.join(data_context.get('categorical_columns', []))}
- ç¼ºå¤±å€¼æƒ…å†µï¼š{data_context.get('missing_values', {})}
- æ•°æ®ç±»å‹ï¼š{data_context.get('dtypes', {})}
"""
            
            # æ„å»ºç³»ç»Ÿæç¤ºï¼ˆæç®€ç‰ˆæœ¬ï¼Œæœ€å¤§åŒ–å“åº”é€Ÿåº¦ï¼‰
            system_prompt = f"""
æ•°æ®åˆ†æä¸“å®¶ã€‚æ•°æ®ä¿¡æ¯ï¼š{context_info}

å›¾è¡¨ï¼šhistogram,scatter,line,bar,pie,heatmap,box,violin

JSONæ ¼å¼ï¼š
{{"analysis":"åˆ†æ","visualization":{{"needed":true/false,"chart_type":"ç±»å‹","columns":["åˆ—"],"title":"æ ‡é¢˜","description":"è¯´æ˜","recommendations":["é€‰é¡¹"],"insights":"æ´å¯Ÿ"}}}}

ç®€æ´ä¸­æ–‡å›ç­”ã€‚
"""
            
            # æ„å»ºæ¶ˆæ¯å†å²
            messages = [
                {
                    "role": "system", 
                    "content": system_prompt
                }
            ]
            
            # æ·»åŠ èŠå¤©å†å²ï¼ˆæœ€è¿‘5è½®å¯¹è¯ï¼‰
            if chat_history:
                recent_history = chat_history[-10:]  # ä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
                for msg in recent_history:
                    messages.append({
                        "role": msg['role'],
                        "content": msg['content']
                    })
            
            # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
            messages.append({
                "role": "user",
                "content": user_question
            })
            
            # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
            import time
            start_time = time.time()
            logging.info(f"å¼€å§‹OpenAI APIè°ƒç”¨ï¼Œæ¨¡å‹: {self.model}")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=float(os.getenv('OPENAI_TEMPERATURE', 0.7)),
                max_tokens=int(os.getenv('OPENAI_MAX_TOKENS', 800)),  # å¢åŠ æœ€å¤§tokenæ•°
                timeout=120  # å¢åŠ åˆ°120ç§’è¶…æ—¶ï¼ŒåŒ¹é…å‰ç«¯è®¾ç½®
            )
            
            # è®°å½•è¯·æ±‚å®Œæˆæ—¶é—´
            end_time = time.time()
            duration = end_time - start_time
            logging.info(f"OpenAI APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            
            ai_response = response.choices[0].message.content
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                clean_response = ai_response.strip()
                if clean_response.startswith('```json'):
                    clean_response = clean_response[7:]
                if clean_response.endswith('```'):
                    clean_response = clean_response[:-3]
                clean_response = clean_response.strip()
                
                parsed_response = json.loads(clean_response)
                
                return {
                    "success": True,
                    "response": parsed_response.get('analysis', ai_response),
                    "visualization": parsed_response.get('visualization', {'needed': False}),
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "structured": True
                }
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬å“åº”
                return {
                    "success": True,
                    "response": ai_response,
                    "visualization": {'needed': False},
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "structured": False
                }
            
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logging.error(f"AIå¯¹è¯å¤±è´¥: {e}")
            logging.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_details}")
            
            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
            if "timeout" in str(e).lower():
                error_msg = "OpenAI APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
            elif "connection" in str(e).lower():
                error_msg = "æ— æ³•è¿æ¥åˆ°OpenAI APIï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            elif "api_key" in str(e).lower() or "unauthorized" in str(e).lower():
                error_msg = "APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥OpenAI APIé…ç½®"
            else:
                error_msg = f"AIåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            
            return {
                "success": False,
                "error": str(e),
                "response": f"ğŸ¤– **{error_msg}**\n\nğŸ’¡ **æç¤ºï¼š** è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIé…ç½®ã€‚"
            }
    
    def analyze_data_insights(self, data_summary: Dict) -> Dict:
        """åˆ†ææ•°æ®æ´å¯Ÿ"""
        if not self.client:
            return {
                "success": False,
                "error": "OpenAI APIæœªé…ç½®",
                "insights": "AIåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥OpenAI APIé…ç½®ã€‚"
            }
        
        try:
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ•°æ®æ‘˜è¦ï¼Œæä¾›æ•°æ®æ´å¯Ÿå’Œå»ºè®®ï¼š
            
            æ•°æ®å½¢çŠ¶: {data_summary.get('shape', 'N/A')}
            åˆ—å: {data_summary.get('columns', [])}
            æ•°æ®ç±»å‹: {data_summary.get('dtypes', {})}
            ç¼ºå¤±å€¼: {data_summary.get('missing_values', {})}
            
            è¯·æä¾›ï¼š
            1. æ•°æ®è´¨é‡è¯„ä¼°
            2. æ½œåœ¨çš„æ•°æ®é—®é¢˜
            3. å»ºè®®çš„å¤„ç†æ­¥éª¤
            4. å¯èƒ½çš„åˆ†ææ–¹å‘
            
            è¯·ä»¥ç»“æ„åŒ–çš„ä¸­æ–‡å›å¤ã€‚
            """
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿æ•°æ®è´¨é‡è¯„ä¼°å’Œæ´å¯Ÿåˆ†æã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                timeout=25  # è®¾ç½®25ç§’è¶…æ—¶
            )
            
            return {
                "success": True,
                "insights": response.choices[0].message.content,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
        except Exception as e:
            logging.error(f"LLMåˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "insights": "åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®"
            }
    
    def generate_analysis_suggestions(self, data_columns: List[str]) -> Dict:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        if not self.client:
            return {
                "success": False,
                "error": "OpenAI APIæœªé…ç½®",
                "suggestions": "AIåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥OpenAI APIé…ç½®ã€‚"
            }
        
        try:
            prompt = f"""
            åŸºäºä»¥ä¸‹æ•°æ®åˆ—ï¼Œå»ºè®®åˆé€‚çš„åˆ†ææ–¹æ³•å’Œå¯è§†åŒ–ç±»å‹ï¼š
            
            æ•°æ®åˆ—: {data_columns}
            
            è¯·ä¸ºæ¯ä¸ªåˆ—æˆ–åˆ—ç»„åˆæ¨èï¼š
            1. é€‚åˆçš„ç»Ÿè®¡åˆ†ææ–¹æ³•
            2. æ¨èçš„å¯è§†åŒ–å›¾è¡¨ç±»å‹
            3. å¯èƒ½çš„ä¸šåŠ¡æ´å¯Ÿè§’åº¦
            
            è¯·ä»¥ç»“æ„åŒ–çš„ä¸­æ–‡å›å¤ã€‚
            """
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®å¯è§†åŒ–å’Œåˆ†æä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                timeout=25  # è®¾ç½®25ç§’è¶…æ—¶
            )
            
            return {
                "success": True,
                "suggestions": response.choices[0].message.content
            }
            
        except Exception as e:
            logging.error(f"å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def explain_chart(self, chart_data: Dict, chart_type: str) -> str:
        """è§£é‡Šå›¾è¡¨å«ä¹‰"""
        if not self.client:
            return "AIåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥OpenAI APIé…ç½®ã€‚"
        
        try:
            prompt = f"""
            è¯·è§£é‡Šä»¥ä¸‹{chart_type}å›¾è¡¨çš„å«ä¹‰å’Œæ´å¯Ÿï¼š
            
            å›¾è¡¨æ•°æ®: {json.dumps(chart_data, ensure_ascii=False)}
            
            è¯·æä¾›ï¼š
            1. å›¾è¡¨æ˜¾ç¤ºçš„ä¸»è¦è¶‹åŠ¿
            2. å…³é”®æ•°æ®ç‚¹çš„è§£é‡Š
            3. å¯èƒ½çš„ä¸šåŠ¡å«ä¹‰
            4. è¿›ä¸€æ­¥åˆ†æå»ºè®®
            """
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®è§£è¯»ä¸“å®¶ï¼Œæ“…é•¿ä»å›¾è¡¨ä¸­æå–ä¸šåŠ¡æ´å¯Ÿã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.6,
                timeout=25  # è®¾ç½®25ç§’è¶…æ—¶
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logging.error(f"å›¾è¡¨è§£é‡Šå¤±è´¥: {e}")
            return f"å›¾è¡¨è§£é‡Šå¤±è´¥: {str(e)}"